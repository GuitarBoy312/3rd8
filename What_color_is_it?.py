import streamlit as st
from openai import OpenAI
import os
from pathlib import Path
from audiorecorder import audiorecorder
from pydub import AudioSegment
from pydub.playback import play
from io import BytesIO

# OpenAI API í‚¤ ì„¤ì •
client = OpenAI(api_key=st.secrets["openai_api_key"])

# ChatGPT API í˜¸ì¶œ
def get_chatgpt_response(prompt):
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼)

# ìŒì„±ì„ ë…¹ìŒí•˜ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def record_and_transcribe():
    audio = audiorecorder("ë…¹ìŒ ì‹œì‘", "ë…¹ìŒ ì™„ë£Œ", pause_prompt="ì ê¹ ë©ˆì¶¤")
    
    if len(audio) > 0:
        st.session_state['last_recording'] = audio  # ë§ˆì§€ë§‰ ë…¹ìŒ ì €ì¥
        st.success("ë…¹ìŒì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë³€í™˜ ì¤‘ì…ë‹ˆë‹¤...")
        st.write("ë‚´ê°€ í•œ ë§ ë“£ê¸°")
        st.audio(audio.export().read()) 
        
        # ë…¹ìŒí•œ ì˜¤ë””ì˜¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        audio_file_path = Path("recorded_audio.wav")
        audio.export(str(audio_file_path), format="wav")

        # Whisper APIë¥¼ ì‚¬ìš©í•´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        with open(audio_file_path, "rb") as audio_file:
            transcription = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file
            )
        return transcription.text
    
    return None

# í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¬ìƒí•˜ëŠ” í•¨ìˆ˜
def text_to_speech_openai(text):
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼)

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []

if 'last_recording' not in st.session_state:
    st.session_state['last_recording'] = None

# ë©”ì‹œì§€ ì¶œë ¥ í•¨ìˆ˜
def display_messages():
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼)

# Streamlit UI

# ë©”ì¸ í™”ë©´ êµ¬ì„±
st.header("âœ¨ì¸ê³µì§€ëŠ¥ ì˜ì–´ëŒ€í™” ì„ ìƒë‹˜ ì‰ê¸€ë§ğŸ‘©â€ğŸ«")
st.markdown("**ğŸ’•ê°ì •ì´ë‚˜ ëŠë‚Œì— ëŒ€í•œ ëŒ€í™”í•˜ê¸°**")
st.divider()

# í™•ì¥ ì„¤ëª…
# ... (ì´ì „ ì½”ë“œì™€ ë™ì¼)

# ë²„íŠ¼ ë°°ì¹˜
col1, col2 = st.columns([1,1])

with col1:
    user_input_text = record_and_transcribe()
    if user_input_text:
        st.session_state['chat_history'].append({"role": "user", "content": user_input_text})
        response = get_chatgpt_response(user_input_text)
        if response:
            text_to_speech_openai(response)
            st.session_state['chat_history'].append({"role": "chatbot", "content": response})    

with col2:
    if st.button("ì²˜ìŒë¶€í„° ë‹¤ì‹œí•˜ê¸°", type="primary"):
        st.session_state['chat_history'] = []
        st.session_state['last_recording'] = None  # ë§ˆì§€ë§‰ ë…¹ìŒ ì´ˆê¸°í™”
        # ë…¹ìŒëœ ìŒì„± íŒŒì¼ ì‚­ì œ
        if os.path.exists("recorded_audio.wav"):
            os.remove("recorded_audio.wav")
        if os.path.exists("speech.mp3"):
            os.remove("speech.mp3")
        st.rerun()

# ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    # ë©”ì‹œì§€ í‘œì‹œ
    display_messages()

# ë§ˆì§€ë§‰ ë…¹ìŒ ì¬ìƒ (í•„ìš”í•œ ê²½ìš°)
if st.session_state['last_recording'] is not None:
    st.audio(st.session_state['last_recording'].export().read())
